{
  "AIME2025I_P2_modified": {
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 307.2,
      "avg_character_count_reasoning": 1346.8,
      "avg_character_count_completion": 1654.0,
      "avg_tokens_output": 172.4,
      "avg_tokens_reasoning": 1459.2,
      "avg_tokens_completions": 1631.6
    },
    "gemini-2.5-pro": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 2305.8,
      "avg_character_count_reasoning": 3454.2,
      "avg_character_count_completion": 5760.0,
      "avg_tokens_output": 969.4,
      "avg_tokens_reasoning": 5134.8,
      "avg_tokens_completions": 6104.2
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1484.0,
      "avg_character_count_reasoning": 795.8,
      "avg_character_count_completion": 2279.8,
      "avg_tokens_output": 7260.8,
      "avg_tokens_reasoning": 218.6,
      "avg_tokens_completions": 7479.4
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1982.4,
      "avg_character_count_reasoning": 1583.6,
      "avg_character_count_completion": 3566.0,
      "avg_tokens_output": 934.8,
      "avg_tokens_reasoning": 1943.4,
      "avg_tokens_completions": 2878.2
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1171.8,
      "avg_character_count_reasoning": 8390.8,
      "avg_character_count_completion": 9562.6,
      "avg_tokens_output": 469.4,
      "avg_tokens_reasoning": 3180.8,
      "avg_tokens_completions": 3650.2
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 796.2,
      "avg_character_count_reasoning": 37605.4,
      "avg_character_count_completion": 38401.6,
      "avg_tokens_output": 5281.6,
      "avg_tokens_reasoning": 10160.8,
      "avg_tokens_completions": 15442.4
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 2057.0,
      "avg_character_count_reasoning": 16975.2,
      "avg_character_count_completion": 19032.2,
      "avg_tokens_output": 3328.6,
      "avg_tokens_reasoning": 4511.2,
      "avg_tokens_completions": 7839.8
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1960.2,
      "avg_character_count_reasoning": 14269.6,
      "avg_character_count_completion": 16229.8,
      "avg_tokens_output": 3025.8,
      "avg_tokens_reasoning": 3754.0,
      "avg_tokens_completions": 6779.8
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1136.0,
      "avg_character_count_reasoning": 5453.4,
      "avg_character_count_completion": 6589.4,
      "avg_tokens_output": 1138.0,
      "avg_tokens_reasoning": 1454.6,
      "avg_tokens_completions": 2592.6
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1791.8,
      "avg_character_count_reasoning": 14496.0,
      "avg_character_count_completion": 16287.8,
      "avg_tokens_output": 2304.4,
      "avg_tokens_reasoning": 3957.2,
      "avg_tokens_completions": 6261.6
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1092.6,
      "avg_character_count_reasoning": 7789.8,
      "avg_character_count_completion": 8882.4,
      "avg_tokens_output": 1100.2,
      "avg_tokens_reasoning": 2070.2,
      "avg_tokens_completions": 3170.4
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 1113.8,
      "avg_character_count_reasoning": 16406.8,
      "avg_character_count_completion": 17520.6,
      "avg_tokens_completions": 7612.8
    },
    "magistral-small-2506": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 41": 1.0
      },
      "avg_character_count_output": 590.2,
      "avg_character_count_reasoning": 31589.0,
      "avg_character_count_completion": 32179.2,
      "avg_tokens_output": 4210.2,
      "avg_tokens_reasoning": 8524.2,
      "avg_tokens_completions": 12734.4
    }
  },
  "AIME2023II_P1": {
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 139.2,
      "avg_character_count_reasoning": 688.0,
      "avg_character_count_completion": 827.2,
      "avg_tokens_output": 72.0,
      "avg_tokens_reasoning": 345.6,
      "avg_tokens_completions": 417.6
    },
    "gemini-2.5-pro": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1673.4,
      "avg_character_count_reasoning": 2098.8,
      "avg_character_count_completion": 3772.2,
      "avg_tokens_output": 611.0,
      "avg_tokens_reasoning": 2016.8,
      "avg_tokens_completions": 2627.8
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1465.2,
      "avg_character_count_reasoning": 981.8,
      "avg_character_count_completion": 2447.0,
      "avg_tokens_output": 1560.4,
      "avg_tokens_reasoning": 271.0,
      "avg_tokens_completions": 1831.4
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 2611.6,
      "avg_character_count_reasoning": 2729.2,
      "avg_character_count_completion": 5340.8,
      "avg_tokens_output": 1140.4,
      "avg_tokens_reasoning": 3472.0,
      "avg_tokens_completions": 4612.4
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1054.2,
      "avg_character_count_reasoning": 4082.6,
      "avg_character_count_completion": 5136.8,
      "avg_tokens_output": 330.0,
      "avg_tokens_reasoning": 1335.0,
      "avg_tokens_completions": 1665.0
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 704.4,
      "avg_character_count_reasoning": 12255.2,
      "avg_character_count_completion": 12959.6,
      "avg_tokens_output": 774.4,
      "avg_tokens_reasoning": 3272.6,
      "avg_tokens_completions": 4047.0
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1811.6,
      "avg_character_count_reasoning": 7966.0,
      "avg_character_count_completion": 9777.6,
      "avg_tokens_output": 1138.2,
      "avg_tokens_reasoning": 2133.8,
      "avg_tokens_completions": 3272.0
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1754.6,
      "avg_character_count_reasoning": 7736.8,
      "avg_character_count_completion": 9491.4,
      "avg_tokens_output": 1273.8,
      "avg_tokens_reasoning": 2055.4,
      "avg_tokens_completions": 3329.2
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 813.2,
      "avg_character_count_reasoning": 2560.8,
      "avg_character_count_completion": 3374.0,
      "avg_tokens_output": 475.6,
      "avg_tokens_reasoning": 679.0,
      "avg_tokens_completions": 1154.6
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1444.2,
      "avg_character_count_reasoning": 11390.4,
      "avg_character_count_completion": 12834.6,
      "avg_tokens_output": 1019.0,
      "avg_tokens_reasoning": 3103.6,
      "avg_tokens_completions": 4122.6
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1297.0,
      "avg_character_count_reasoning": 5259.4,
      "avg_character_count_completion": 6556.4,
      "avg_tokens_output": 615.6,
      "avg_tokens_reasoning": 1366.6,
      "avg_tokens_completions": 1982.2
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 1175.4,
      "avg_character_count_reasoning": 4376.6,
      "avg_character_count_completion": 5552.0,
      "avg_tokens_completions": 2082.6
    },
    "magistral-small-2506": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 220": 1.0
      },
      "avg_character_count_output": 664.2,
      "avg_character_count_reasoning": 13815.0,
      "avg_character_count_completion": 14479.2,
      "avg_tokens_output": 916.6,
      "avg_tokens_reasoning": 3704.6,
      "avg_tokens_completions": 4621.2
    }
  },
  "AIME2023II_P1_mod": {
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 284.2,
      "avg_character_count_reasoning": 900.0,
      "avg_character_count_completion": 1184.2,
      "avg_tokens_output": 149.8,
      "avg_tokens_reasoning": 422.4,
      "avg_tokens_completions": 572.2
    },
    "gemini-2.5-pro": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1489.4,
      "avg_character_count_reasoning": 1925.2,
      "avg_character_count_completion": 3414.6,
      "avg_tokens_output": 549.0,
      "avg_tokens_reasoning": 1953.2,
      "avg_tokens_completions": 2502.2
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1554.8,
      "avg_character_count_reasoning": 994.4,
      "avg_character_count_completion": 2549.2,
      "avg_tokens_output": 2378.2,
      "avg_tokens_reasoning": 271.8,
      "avg_tokens_completions": 2650.0
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 2804.8,
      "avg_character_count_reasoning": 2168.0,
      "avg_character_count_completion": 4972.8,
      "avg_tokens_output": 1226.4,
      "avg_tokens_reasoning": 2979.8,
      "avg_tokens_completions": 4206.2
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 909.2,
      "avg_character_count_reasoning": 4663.4,
      "avg_character_count_completion": 5572.6,
      "avg_tokens_output": 283.4,
      "avg_tokens_reasoning": 1647.2,
      "avg_tokens_completions": 1930.6
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 765.6,
      "avg_character_count_reasoning": 21305.6,
      "avg_character_count_completion": 22071.2,
      "avg_tokens_output": 1303.2,
      "avg_tokens_reasoning": 5654.6,
      "avg_tokens_completions": 6957.8
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1999.8,
      "avg_character_count_reasoning": 9267.6,
      "avg_character_count_completion": 11267.4,
      "avg_tokens_output": 1261.2,
      "avg_tokens_reasoning": 2469.6,
      "avg_tokens_completions": 3730.8
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1613.2,
      "avg_character_count_reasoning": 7601.2,
      "avg_character_count_completion": 9214.4,
      "avg_tokens_output": 1134.4,
      "avg_tokens_reasoning": 1995.6,
      "avg_tokens_completions": 3130.0
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1090.4,
      "avg_character_count_reasoning": 3369.8,
      "avg_character_count_completion": 4460.2,
      "avg_tokens_output": 610.8,
      "avg_tokens_reasoning": 912.6,
      "avg_tokens_completions": 1523.4
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1338.2,
      "avg_character_count_reasoning": 11193.6,
      "avg_character_count_completion": 12531.8,
      "avg_tokens_output": 961.2,
      "avg_tokens_reasoning": 3044.0,
      "avg_tokens_completions": 4005.2
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 1223.2,
      "avg_character_count_reasoning": 3258.8,
      "avg_character_count_completion": 4482.0,
      "avg_tokens_output": 575.6,
      "avg_tokens_reasoning": 873.6,
      "avg_tokens_completions": 1449.2
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 970.4,
      "avg_character_count_reasoning": 3639.8,
      "avg_character_count_completion": 4610.2,
      "avg_tokens_completions": 1754.8
    },
    "magistral-small-2506": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 660": 1.0
      },
      "avg_character_count_output": 679.8,
      "avg_character_count_reasoning": 19160.6,
      "avg_character_count_completion": 19840.4,
      "avg_tokens_output": 1403.6,
      "avg_tokens_reasoning": 5117.0,
      "avg_tokens_completions": 6520.6
    }
  }
}