{
  "AIME2025I_Problem_2": {
    "o4-mini-high-long": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 224.2,
      "avg_character_count_reasoning": 1340.8,
      "avg_character_count_completion": 1565.0,
      "avg_tokens_output": 136.6,
      "avg_tokens_reasoning": 1369.6,
      "avg_tokens_completions": 1506.2
    },
    "gemini-2.5-pro": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 2111.8,
      "avg_character_count_reasoning": 3146.8,
      "avg_character_count_completion": 5258.6,
      "avg_tokens_output": 868.4,
      "avg_tokens_reasoning": 4589.0,
      "avg_tokens_completions": 5457.4
    },
    "claude-4-sonnet-0522-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1419.4,
      "avg_character_count_reasoning": 769.8,
      "avg_character_count_completion": 2189.2,
      "avg_tokens_output": 4907.8,
      "avg_tokens_reasoning": 210.4,
      "avg_tokens_completions": 5118.2
    },
    "gemini-2.5-flash-high": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 2235.2,
      "avg_character_count_reasoning": 1781.4,
      "avg_character_count_completion": 4016.6,
      "avg_tokens_output": 1093.2,
      "avg_tokens_reasoning": 2457.8,
      "avg_tokens_completions": 3551.0
    },
    "grok-3-mini-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 909.0,
      "avg_character_count_reasoning": 8856.6,
      "avg_character_count_completion": 9765.6,
      "avg_tokens_output": 351.2,
      "avg_tokens_reasoning": 3416.4,
      "avg_tokens_completions": 3767.6
    },
    "magistral-medium-2506-thinking": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 782.6,
      "avg_character_count_reasoning": 31423.8,
      "avg_character_count_completion": 32206.4,
      "avg_tokens_output": 4224.4,
      "avg_tokens_reasoning": 8466.6,
      "avg_tokens_completions": 12691.0
    },
    "qwen3-235b-a22b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1924.6,
      "avg_character_count_reasoning": 20168.4,
      "avg_character_count_completion": 22093.0,
      "avg_tokens_output": 4208.4,
      "avg_tokens_reasoning": 5320.4,
      "avg_tokens_completions": 9528.8
    },
    "qwen3-30b-a3b-think": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1813.0,
      "avg_character_count_reasoning": 14076.4,
      "avg_character_count_completion": 15889.4,
      "avg_tokens_output": 2957.8,
      "avg_tokens_reasoning": 3734.0,
      "avg_tokens_completions": 6691.8
    },
    "glm-z1-32b": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1074.8,
      "avg_character_count_reasoning": 5863.2,
      "avg_character_count_completion": 6938.0,
      "avg_tokens_output": 1043.2,
      "avg_tokens_reasoning": 1569.4,
      "avg_tokens_completions": 2612.6
    },
    "deepseek-r1-0528": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1639.2,
      "avg_character_count_reasoning": 18852.8,
      "avg_character_count_completion": 20492.0,
      "avg_tokens_output": 2515.0,
      "avg_tokens_reasoning": 5158.4,
      "avg_tokens_completions": 7673.4
    },
    "deepseek-r1": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1054.6,
      "avg_character_count_reasoning": 6637.8,
      "avg_character_count_completion": 7692.4,
      "avg_tokens_output": 915.6,
      "avg_tokens_reasoning": 1758.4,
      "avg_tokens_completions": 2674.0
    },
    "DeepHermes-3-Mistral-24B-Pre": {
      "average_total_score": 1.0,
      "num_evaluations": 5,
      "criteria_stats": {
        "Answer is 49": 1.0
      },
      "avg_character_count_output": 1121.2,
      "avg_character_count_reasoning": 17421.6,
      "avg_character_count_completion": 18542.8,
      "avg_tokens_completions": 7966.4
    }
  }
}